{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab69cbd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T16:45:45.295676Z",
     "start_time": "2023-12-28T16:45:45.224500Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afb75a",
   "metadata": {},
   "source": [
    "# Loading the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91fe36d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T16:36:15.859448Z",
     "start_time": "2023-12-28T16:36:15.843445Z"
    }
   },
   "outputs": [],
   "source": [
    "class finish_layers(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(finish_layers, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\") # Call's distilbert model\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768) # add aditional layers for prob outputs\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b3c21f",
   "metadata": {},
   "source": [
    " # Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c2db56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T16:47:14.914698Z",
     "start_time": "2023-12-28T16:47:13.154510Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "finish_layers(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model using the saved parameters \n",
    "input_model_file = './models/model.pt'\n",
    "model = finish_layers()\n",
    "model.load_state_dict(torch.load(input_model_file))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3064147b",
   "metadata": {},
   "source": [
    "# Load the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a168c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T16:48:23.853027Z",
     "start_time": "2023-12-28T16:48:23.782010Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1714: FutureWarning: Calling DistilBertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "input_vocab_file = './models/vocab_distilbert_twitter.bin'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(input_vocab_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a80d66",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12b483de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T17:02:39.618015Z",
     "start_time": "2023-12-28T17:02:39.600011Z"
    }
   },
   "outputs": [],
   "source": [
    "# A function to adjust the input and make the inference\n",
    "def inference(model, phrase,tokenizer):\n",
    "    model.eval()\n",
    "    title = str(phrase)\n",
    "    title = \" \".join(title.split())\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        title,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=160,\n",
    "        pad_to_max_length=True,\n",
    "        return_token_type_ids=True,\n",
    "        truncation=True\n",
    "        )\n",
    "    ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "    value_tensor={'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                  'mask': torch.tensor(mask, dtype=torch.long)}\n",
    "    with torch.no_grad():\n",
    "            ids = value_tensor['ids'].to(dtype = torch.long)\n",
    "            mask = value_tensor['mask'].to(dtype = torch.long)\n",
    "            outputs = model(ids.unsqueeze(0), mask.unsqueeze(0))\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48120afa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T17:16:56.074467Z",
     "start_time": "2023-12-28T17:16:55.939437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your phrase is negative!!\n"
     ]
    }
   ],
   "source": [
    "phrase = \"I didn't like the food of this restaurant\"\n",
    "output = inference(model,phrase,tokenizer)\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "# Get the predicted class (index with maximum probability)\n",
    "predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "if predicted_class == 1:\n",
    "    print(\"Your phrase is positive!!\")\n",
    "elif predicted_class == 0:\n",
    "     print(\"Your phrase is negative!!\")\n",
    "else: \n",
    "    print(\"error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
